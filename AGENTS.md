# Repository Guidelines

This repo runs 3 DeepStream containers (s0, s1, s2) for YOLOv8 inference on live RTSP streams from a MediaMTX relay.

## Architecture

### Current Setup (3 Live Streams)
- **ds-s0**: Pulls from `rtsp://RELAY_IP:8554/in_s0`, serves on `localhost:8554/ds-test`
- **ds-s1**: Pulls from `rtsp://RELAY_IP:8554/in_s1`, serves on `localhost:8555/ds-test`
- **ds-s2**: Pulls from `rtsp://RELAY_IP:8554/in_s2`, serves on `localhost:8556/ds-test`

All 3 use **single parameterized C binary** (`live_stream`) that takes stream ID (0, 1, 2) as argument.

### Data Flow
```
Cameras → Relay (in_s0, in_s1, in_s2)
         ↓
DeepStream pulls from relay → YOLOv8 inference → Local RTSP (8554, 8555, 8556)
         ↓
frpc tunnels localhost → Relay (9500, 9501, 9502)
         ↓
Relay pulls from tunnels → Serves as s0, s1, s2 (WebRTC/HLS/RTSP)
```

### Key Components

**MediaMTX Relay** (managed by Terraform in `relay/`):
- **Current IP**: `34.47.221.242`
- **Inputs**: `in_s0`, `in_s1`, `in_s2` (cameras publish here)
- **Outputs**: `s0`, `s1`, `s2` (pulls from localhost:9500-9502 via frp tunnels)
- **Config**: `/etc/mediamtx/config.yml` (generated by `relay/scripts/startup.sh`)

**frp Tunneling**:
- **frps** (server): Runs on relay, accepts tunnels on port 7000
- **frpc** (client): Runs locally, tunnels localhost:8554-8556 → relay:9500-9502
- **Config**: `config/frpc.ini` (contains relay IP and token)

**DeepStream Containers**:
- All use YOLOv8n (`config/config_infer_yolov8.txt`)
- All use batch-size=1 for single-stream inference
- All built from same Docker image with single parameterized binary
- Launched with different stream IDs: `/app/live_stream 0`, `/app/live_stream 1`, `/app/live_stream 2`

**Publisher** (test video streaming):
- Isolated in `publisher/` folder
- Runs in separate container
- Publishes test video to `in_s2` for testing without cameras

## Critical Files Containing Relay IP

When the relay IP changes, update ALL of these:
1. `live_stream.c` (line 97: `snprintf(input_uri, ...)`)
2. `config/frpc.ini` (line 2: `server_addr`)
3. `publisher/loop_stream.sh` (line 7: `rtspclientsink location`)

**Much simpler now**: Only 3 files instead of 6 thanks to parameterized binary.

## Relay Changes (Terraform Workflow)

The relay is **infrastructure as code**. Changes to `relay/scripts/startup.sh` require recreating the VM.

### To Update Relay Config
```bash
cd relay/

# 1. Make changes to relay/scripts/startup.sh
# 2. Destroy and recreate (IP will change unless you have static IP)
export GOOGLE_OAUTH_ACCESS_TOKEN=$(gcloud auth print-access-token)
terraform destroy -var project_id=fsp-api-1 -auto-approve
terraform apply -var project_id=fsp-api-1 -auto-approve

# 3. Note the new external_ip from output
terraform output external_ip

# 4. Get the new frps_token
terraform output -raw frps_token
```

### After Relay IP Change
```bash
# 1. Update all 3 files listed above with new IP and token
# 2. Rebuild Docker images
./build.sh

# 3. Restart frpc with new config
pkill frpc
nohup frpc -c /root/d_final/config/frpc.ini > /var/log/frpc.log 2>&1 &

# 4. Verify frpc connected
tail -10 /var/log/frpc.log
# Should see: "proxy added: [s0_rtsp s1_rtsp s2_rtsp]"

# 5. Restart containers
./start.sh

# 6. Start publisher (if using test video for in_s2)
cd publisher/
./start.sh
```

## frpc (Local FRP Client)

**What it does**: Tunnels DeepStream's local RTSP servers to the relay so viewers can access processed streams.

**When to restart**:
- After changing `config/frpc.ini` (IP or token change)
- If relay was recreated
- If tunnels disconnect (check logs)

**How to restart**:
```bash
pkill frpc
nohup frpc -c /root/d_final/config/frpc.ini > /var/log/frpc.log 2>&1 &
```

**How to check status**:
```bash
tail -20 /var/log/frpc.log
# Good: "login to server success", "proxy added: [s0_rtsp s1_rtsp s2_rtsp]"
# Bad: "dial tcp ... i/o timeout" (wrong IP), "authorization failed" (wrong token)
```

## Quick Workflow

### Start everything
```bash
./build.sh    # Rebuild if live_stream.c changed
./start.sh    # Start all: ds-s0, ds-s1, ds-s2, publisher
```

### Stop everything
```bash
./stop.sh     # Stop all containers
```

### Check status
```bash
# Local containers
docker ps | grep ds-s

# Local RTSP servers
ffprobe rtsp://127.0.0.1:8554/ds-test  # s0
ffprobe rtsp://127.0.0.1:8555/ds-test  # s1
ffprobe rtsp://127.0.0.1:8556/ds-test  # s2

# frpc tunnels
tail -10 /var/log/frpc.log

# Relay status
gcloud compute ssh mediamtx-relay --zone=asia-south1-c \
  --command="docker logs mediamtx --tail 20"
```

### View processed streams
- s0: `http://34.47.221.242:8889/s0/`
- s1: `http://34.47.221.242:8889/s1/`
- s2: `http://34.47.221.242:8889/s2/`

### View input streams (before processing)
- in_s0: `http://34.47.221.242:8889/in_s0/`
- in_s1: `http://34.47.221.242:8889/in_s1/`
- in_s2: `http://34.47.221.242:8889/in_s2/`

## Swapping Models

To change from YOLOv8n to another model:
```bash
# Edit inference config path in live_stream.c
sed -i 's|config_infer_yolov8.txt|config_infer_new.txt|' live_stream.c

# Rebuild
./build.sh

# Clear old engines
rm models/*.engine

# Restart
./start.sh
```

## TensorRT Engine Management

### First Build (No Cached Engine)
**IMPORTANT**: Avoid race conditions when building engines for the first time:
```bash
# Stop all containers
docker stop ds-s0 ds-s1 ds-s2

# Start ONLY s0 to build engine
docker start ds-s0

# Wait ~3-5 minutes for engine build
# Watch logs: docker logs ds-s0 -f

# Once complete, restart all containers
./start.sh
```

### Rebuilding Engines
When dimensions or model changes:
```bash
rm models/*.engine    # Clear cached engines
./start.sh            # All containers use shared engine file
```

## Performance

All 3 streams process at ~30 FPS with YOLOv8n. Check with:
```bash
docker logs ds-s0 | grep "**PERF"
docker logs ds-s1 | grep "**PERF"
docker logs ds-s2 | grep "**PERF"
```

GPU memory usage per container: ~390 MiB
```bash
nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv
```

## Recent Cleanup (2025-10-20)

### What Changed
1. **DRY principle**: Replaced 3 duplicate C files (`live_s0.c`, `live_s1.c`, `live_s2.c`) with single parameterized `live_stream.c`
2. **Publisher organized**: Moved all publisher files to `publisher/` folder with own Dockerfile and start.sh
3. **Removed legacy code**: Deleted unused Python version, old test files, duplicate scripts
4. **Documentation updated**: All 3 docs (STRUCTURE.md, STANDARDS.md, AGENTS.md) reflect new architecture

### Benefits
- **Simpler IP updates**: 3 files instead of 6
- **Easier maintenance**: One binary, one source file
- **Better organization**: Publisher isolated in own folder
- **Less confusion**: No duplicate/legacy code lying around

### Migration Notes
If you have old code referencing:
- `live_s0`, `live_s1`, `live_s2` binaries → Use `/app/live_stream 0/1/2` instead
- `Dockerfile.publisher` → Now `publisher/Dockerfile`
- `loop_stream.sh` → Now `publisher/loop_stream.sh`
- `s0_rtsp.py` → Deleted (was legacy Python version)

## Expectations for Future Edits

1. **IP changes less disruptive**: Only 3 files to update (was 6)
2. **Relay is immutable**: Changes to `relay/scripts/startup.sh` require terraform destroy/apply
3. **frpc is critical**: Without it, relay can't pull processed streams
4. **All 3 streams identical**: Same binary, same config, just different stream ID
5. **Don't modify s0/s1 when debugging s2**: They work, leave them alone
6. **TensorRT engine caching**: Build once, share across all containers (but watch for race conditions on first build)

## Portrait Video Implementation Checklist

Before starting portrait video work, ensure baseline is stable by running:
```bash
./system.sh status    # All checks should pass
git status            # Should be clean
```

### Phase 1: Research & Planning
- [ ] Study NVIDIA DeepStream portrait video handling
- [ ] Test current setup with portrait input (publisher already has portrait video)
- [ ] Document observed issues (aspect ratio, bounding boxes, etc.)
- [ ] Identify required nvstreammux/nvosd/encoder changes

### Phase 2: Configuration Changes
- [ ] Update nvstreammux dimensions in live_stream.c (currently 1920x1080)
- [ ] Test with maintain-aspect-ratio=0 vs maintain-aspect-ratio=1
- [ ] Adjust OSD properties for portrait coordinates
- [ ] Update encoder settings if needed (iframeinterval, bitrate)
- [ ] Test TensorRT engine rebuild with new dimensions

### Phase 3: Testing & Validation
- [ ] Verify FPS remains ~30 with portrait video
- [ ] Check bounding box coordinates are correct
- [ ] Verify GPU memory usage is acceptable
- [ ] Test all 3 streams with portrait input
- [ ] Check WebRTC playback on relay

### Phase 4: Rollback Plan
If portrait implementation fails:
```bash
git reset --hard pre-portrait-stable  # Reset to stable tag
./build.sh                             # Rebuild
./system.sh restart                    # Restart
```

### Key Risks
1. **Aspect ratio handling**: maintain-aspect-ratio=1 adds padding, =0 may distort
2. **TensorRT engine size**: Portrait may require different input dimensions
3. **Coordinate mapping**: YOLOv8 bbox coords may need adjustment for portrait
4. **Performance**: Different resolution may impact FPS

### Success Criteria
- [ ] All 3 streams process portrait video at 30 FPS
- [ ] Bounding boxes correctly positioned on people
- [ ] No GPU memory issues
- [ ] WebRTC playback works on relay
- [ ] System remains stable after 10 minutes of operation
