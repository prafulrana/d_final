# Repository Guidelines

This repo runs 4 DeepStream containers (s0, s1, s2, s3) for YOLOv8 inference on live RTSP streams from a MediaMTX relay.

## ⚠️ IMPORTANT: Always Use Managed Scripts

**DO NOT use raw docker/mediamtx/frp commands**. This repo provides managed lifecycle scripts that handle system state correctly.

**ALWAYS use these managed scripts**:
- `./system` - Full system management (frpc + DeepStream)
- `./ds` - DeepStream containers only
- `./relay` - Remote relay management
- `./.scripts/debug.sh` - Debugging diagnostics
- `./.scripts/check.sh` - Health validation

**Only use raw commands if explicitly documented** in these scripts or if the managed scripts don't provide what you need.

## Component Navigation

### Three Main Components

**1. DeepStream Containers** (Core System)
- **Location**: Managed by `./ds` and `./system` scripts
- **What**: Four containers (ds-s0, ds-s1, ds-s2, ds-s3) running YOLOv8 inference
- **Start**: `./system start` or `./ds start`
- **Stop**: `./system stop` or `./ds stop`
- **Status**: `./ds status`
- **Debug**: `./.scripts/debug.sh`

**2. FRP Tunnel + Relay** (Connectivity)
- **Local (frpc)**: Managed by `./system` script, config in `.scripts/frpc/frpc.ini`
- **Remote (relay)**: Managed by `./relay` script, runs on GCP VM
- **What**: Tunnels local RTSP to public relay for WebRTC streaming
- **Start**: `./system start` (starts frpc)
- **Check**: `./system status` or `./relay status`

## Architecture

### Current Setup (4 Live Streams)
- **ds-s0**: Pulls from `rtsp://RELAY_IP:8554/in_s0`, serves on `localhost:8554/ds-test`
- **ds-s1**: Pulls from `rtsp://RELAY_IP:8554/in_s1`, serves on `localhost:8555/ds-test`
- **ds-s2**: Pulls from `rtsp://RELAY_IP:8554/in_s2`, serves on `localhost:8556/ds-test`
- **ds-s3**: Pulls from `rtsp://RELAY_IP:8554/in_s3`, serves on `localhost:8557/ds-test`

All 4 use **single parameterized C binary** (`live_stream`) that takes stream ID (0, 1, 2, 3) and orientation as arguments.

### Data Flow
```
Cameras → Relay (in_s0, in_s1, in_s2, in_s3)
         ↓
DeepStream pulls from relay → YOLOv8 inference → Local RTSP (8554, 8555, 8556, 8557)
         ↓
frpc tunnels localhost → Relay (9500, 9501, 9502, 9503)
         ↓
Relay pulls from tunnels → Serves as s0, s1, s2, s3 (WebRTC/HLS/RTSP)
```

### Key Components

**MediaMTX Relay** (managed by Terraform in `relay_infra/`):
- **Current IP**: `34.47.221.242`
- **Inputs**: `in_s0`, `in_s1`, `in_s2`, `in_s3` (cameras publish here)
- **Outputs**: `s0`, `s1`, `s2`, `s3` (pulls from localhost:9500-9503 via frp tunnels)
- **Config**: `/etc/mediamtx/config.yml` (generated by `relay_infra/scripts/startup.sh`)
- **Manage**: Use `./relay status` or `./relay restart` (DO NOT ssh manually)

**frp Tunneling**:
- **frps** (server): Runs on relay, accepts tunnels on port 7000
- **frpc** (client): Runs locally, tunnels localhost:8554-8557 → relay:9500-9503
- **Config**: `.scripts/frpc/frpc.ini` (contains relay IP and token)
- **Manage**: Use `./system start/stop/restart` (DO NOT run frpc manually)

**DeepStream Containers**:
- All use YOLOv8n COCO by default (`config/config_infer_yolov8.txt`)
- All use portrait mode (720x1280) by default
- All use batch-size=1 for single-stream inference
- All built from same Docker image with single parameterized binary
- Launched with different stream IDs: `/app/live_stream 0 portrait`, `/app/live_stream 1 portrait`, etc.

## Critical Files Containing Relay IP

When the relay IP changes, update ALL of these:
1. `live_stream.c` (line 96: `snprintf(input_uri, ...)`)
2. `.scripts/frpc/frpc.ini` (line 2: `server_addr`, line 4: `token`)

**Much simpler now**: Only 2 files instead of 6 thanks to parameterized binary.

## Relay Changes (Terraform Workflow)

The relay is **infrastructure as code**. Changes to `relay/scripts/startup.sh` require recreating the VM.

### To Update Relay Config
```bash
cd relay/

# 1. Make changes to relay/scripts/startup.sh
# 2. Destroy and recreate (IP will change unless you have static IP)
export GOOGLE_OAUTH_ACCESS_TOKEN=$(gcloud auth print-access-token)
terraform destroy -var project_id=fsp-api-1 -auto-approve
terraform apply -var project_id=fsp-api-1 -auto-approve

# 3. Note the new external_ip from output
terraform output external_ip

# 4. Get the new frps_token
terraform output -raw frps_token
```

### After Relay IP Change
```bash
# 1. Update all 3 files listed above with new IP and token
# 2. Rebuild Docker images
./build.sh

# 3. Restart entire system (frpc + containers)
./system restart

# 4. Verify everything started
./system status
```

## frpc (Local FRP Client)

**What it does**: Tunnels DeepStream's local RTSP servers to the relay so viewers can access processed streams.

**When to restart**:
- After changing `.scripts/frpc/frpc.ini` (IP or token change)
- If relay was recreated
- If tunnels disconnect (check logs)

**How to restart**:
```bash
./system restart    # Preferred: Restarts frpc + containers together
```

**How to check status**:
```bash
./system status     # Full health check
# Or manually check logs:
tail -20 /var/log/frpc.log
# Good: "login to server success", "proxy added: [s0_rtsp s1_rtsp s2_rtsp]"
# Bad: "dial tcp ... i/o timeout" (wrong IP), "authorization failed" (wrong token)
```

## Quick Workflow

### Start Everything
```bash
./build.sh         # Rebuild if live_stream.c changed
./system start     # Start frpc + all DeepStream containers
```

### Stop Everything
```bash
./system stop      # Stop all containers + frpc
```

### Check Status
```bash
./system status        # Full health check (frpc, DS, relay) - USE THIS FIRST
./ds status            # Just container status
./relay status         # Just relay status
./.scripts/debug.sh    # Detailed diagnostics
```

### Debug Issues
```bash
./.scripts/debug.sh    # ALWAYS run this first for debugging
# Only check individual logs if debug script tells you to
```

### View processed streams
- s0: `http://34.47.221.242:8889/s0/`
- s1: `http://34.47.221.242:8889/s1/`
- s2: `http://34.47.221.242:8889/s2/`
- s3: `http://34.47.221.242:8889/s3/`

### View input streams (before processing)
- in_s0: `http://34.47.221.242:8889/in_s0/`
- in_s1: `http://34.47.221.242:8889/in_s1/`
- in_s2: `http://34.47.221.242:8889/in_s2/`
- in_s3: `http://34.47.221.242:8889/in_s3/`

## Swapping Models

To change from YOLOv8n to another model:
```bash
# 1. Create new config pointing to new ONNX
cp config/config_infer_yolov8.txt config/config_infer_new.txt
vim config/config_infer_new.txt
# Update: onnx-file=/models/new_model.onnx
# Update: model-engine-file=/models/new_model_b1_gpu0_fp16.engine

# 2. Edit inference config path in live_stream.c
sed -i 's|config_infer_yolov8.txt|config_infer_new.txt|' live_stream.c

# 3. Rebuild and restart
./build.sh
./ds restart

# DeepStream will auto-build the new engine (no need to delete old ones)
```

## TensorRT Engine Management

### Engine Caching (CRITICAL)
**NEVER DELETE .engine FILES** - TensorRT engines are cached in `/root/d_final/models/` which is bind-mounted to `/models/` inside containers.

- **Engines persist across container restarts** - this is by design
- **DeepStream auto-rebuilds when needed** - if ONNX changes or engine is missing
- **Building takes 3-5 minutes** - avoid unnecessary rebuilds

### How Engine Caching Works
```
Host: /root/d_final/models/*.engine
      ↓ (bind mount)
Container: /models/*.engine
      ↓ (config points to)
config/config_infer_*.txt: model-engine-file=/models/xxx.engine
```

When you restart containers, engines are already there - no rebuild needed.

### When Engines Auto-Rebuild
DeepStream automatically rebuilds engines when:
1. Engine file doesn't exist at the path in config
2. ONNX model timestamp is newer than engine timestamp
3. GPU architecture changes (different machine)

### Swapping Models (No Manual Engine Deletion Needed)
```bash
# 1. Copy new ONNX to models/
cp /path/to/new_model.onnx /root/d_final/models/

# 2. Update config to point to new ONNX and engine path
vim config/config_infer_bowling.txt
# Change: onnx-file=/models/new_model.onnx
# Change: model-engine-file=/models/new_model_b1_gpu0_fp16.engine

# 3. Restart container - engine builds automatically if missing
./ds restart

# DeepStream will see the new ONNX, notice no engine exists, and build it
```

### Only Delete Engines If
1. **Debugging engine corruption** - engine crashes on load
2. **Forcing rebuild for testing** - want to verify ONNX export is correct
3. **Disk space cleanup** - removing unused old engines

**In these cases only**: `rm /root/d_final/models/specific_model.engine`

## Performance

All 4 streams process at ~30 FPS with YOLOv8n in portrait mode (720x1280). Check with:
```bash
./.scripts/debug.sh    # Shows GPU usage and container status
```

GPU memory usage per container: ~390 MiB

## Recent Changes

### Major Cleanup (2025-10-20)
1. **DRY principle**: Replaced 3 duplicate C files with single parameterized `live_stream.c`
2. **Script consolidation**: Simplified 11 scripts down to 4 (`system`, `ds`, `relay`, `build.sh`)
3. **Utilities hidden**: Moved check.sh, debug.sh, frpc config to `.scripts/`
4. **Publisher removed**: All publisher code removed from codebase (use external publisher for testing)

### Benefits
- **Simpler IP updates**: 2 files instead of 6
- **Easier management**: `./system start` for everything, `./ds start` for containers only
- **Clean separation**: Core system only, test tools external

### Component Separation
- **Production system**: `/root/d_final/` - frpc + DeepStream (s0, s1, s2, s3) - managed by `./system`
- **Training system**: `/root/d_final/training/` - model training and ONNX export
- **Relay**: Remote infrastructure - managed by `./relay`

### Migration Notes
If you have old code referencing:
- `live_s0/s1/s2` binaries → Use `/app/live_stream 0/1/2` instead
- `start.sh/stop.sh` → Use `./system start/stop` or `./ds start/stop`
- `frpc/frpc.ini` → Now `.scripts/frpc/frpc.ini`
- `check.sh/debug.sh` → Now `.scripts/check.sh` and `.scripts/debug.sh`

## Expectations for AI Agents

1. **ALWAYS use managed scripts**: `./system`, `./ds`, `./relay`, `./.scripts/debug.sh` - NOT raw docker/frp/mediamtx commands
2. **Check status before debugging**: Run `./system status` and `./.scripts/debug.sh` before investigating issues
3. **Relay is immutable**: Changes to `relay_infra/scripts/startup.sh` require terraform destroy/apply
4. **frpc is critical**: Without it, relay can't pull processed streams - manage via `./system`
5. **All 4 streams identical**: Same binary, same config, just different stream ID (0-3) and orientation
6. **Don't modify working streams**: If s0-s2 work but s3 doesn't, leave s0-s2 alone
7. **NEVER delete TensorRT engines**: Engines are cached in `/models/` (bind-mounted from host) and persist across restarts - DeepStream auto-rebuilds when needed
8. **Training separation**: Production code in `/root/d_final/`, training experiments in `/root/d_final/training/`

## Training Workflow

### Separation of Concerns
- **Production**: `/root/d_final/` - deployed code, managed via `./system`, `./ds`, `./relay`
- **Training**: `/root/d_final/training/` - model training, ONNX export, experiments
- **Models**: `/root/d_final/models/` - production ONNX files only

### Training Directory Structure
```
training/
├── train_bowling.py              # Training script
├── Bowling-Pin-Detection--4/     # Roboflow dataset (868 images, 1 class)
│   ├── train/images/
│   ├── valid/images/
│   └── data.yaml
└── roboflow_bowling/             # Training runs
    ├── bowling_1280_m_b8/        # YOLOv8m, batch=8, 1280x1280
    │   └── weights/
    │       ├── best.pt           # 92.7% precision, 88.9% recall
    │       └── best.onnx         # Exported ONNX
    └── ...other experiments
```

### Adding New Models
```bash
# 1. Train in /root/d_final/training/
cd /root/d_final/training
python3 train_bowling.py --imgsz 1280 --batch 8 --epochs 150

# 2. Export ONNX with FIXED dimensions (dynamic=False)
docker run --rm --gpus all -v /root/d_final/training:/data \
  ultralytics/ultralytics:latest yolo export \
  model=/data/roboflow_bowling/bowling_1280_m_b8/weights/best.pt \
  format=onnx imgsz=1280 dynamic=False simplify=True

# 3. Copy to production models/
cp training/roboflow_bowling/bowling_1280_m_b8/weights/best.onnx \
   models/bowling_roboflow_1280.onnx

# 4. Create or update config
cp config/config_infer_yolov8.txt config/config_infer_bowling.txt
vim config/config_infer_bowling.txt
# Update: onnx-file=/models/bowling_roboflow_1280.onnx
# Update: model-engine-file=/models/bowling_roboflow_1280_b1_gpu0_fp16.engine
# Update: num-detected-classes=1
# Update: labelfile-path=/models/bowling_labels.txt

# 5. Restart container - engine builds automatically
./ds restart

# NO need to delete engines - DeepStream auto-rebuilds when missing
```

### Current Models
- **s0, s1**: YOLOv8n COCO (80 classes) - 720x1280 portrait
- **s2**: YOLOv8n COCO (80 classes) - 720x1280 portrait
- **s3**: YOLOv8m Bowling Roboflow (1 class) - 720x1280 portrait, 92.7% precision

### Training Lessons Learned
- **Use dynamic=False**: DeepStream requires fixed input dimensions, not variable
- **Batch size matters**: batch=8 works with concurrent DeepStream; batch=16 needs more GPU memory
- **Early stopping works**: YOLOv8m reached 92.7% precision at epoch 70 with patience=20
- **Roboflow datasets**: Company datasets often better than auto-labeled custom data
- **Export format**: Ultralytics CLI export, not DeepStream-Yolo scripts (simpler, works well)
